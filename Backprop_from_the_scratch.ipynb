{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to initialize simple ANN with layers organized as arrays of dictionaries\n",
    "# We start with one hidden layers for the beginning\n",
    "\n",
    "def ANN(n_inputs, n_hidden, n_outputs):\n",
    "    \n",
    "    network = list()\n",
    "    \n",
    "    hidden_layer = [{'weights': [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "\n",
    "    network.append(hidden_layer)\n",
    "    \n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    \n",
    "    network.append(output_layer)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "network = ANN(3, 2, 3)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in network:\n",
    "    \n",
    "    for neuron in layer:\n",
    "        \n",
    "        print(neuron['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagate\n",
    "'To calculate an output from a neural network by propagating an input signal through each layer until the output layer outputs its values. We call this forward-propagation. Forward prop can be separated into three parts: 1) Neuron activation, 2) Neuron Transfer, 3) Forward Propagation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron Activation\n",
    "'The first step is to calculate the activation of one neuron given an input.'\n",
    "\n",
    "def activate(w, inputs):\n",
    "    \n",
    "    activation = w[-1] # bias\n",
    "    \n",
    "    for i in range(len(w)-1):\n",
    "        \n",
    "        activation += w[i]*inputs[i] # computing sum over all products of weights multiplied by input  \n",
    "    \n",
    "        return activation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-linearity  we are going to use is ReLU\n",
    "\n",
    "def ReLu(activation):\n",
    "    \n",
    "    return activation *(activation > 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "'We need to calculate an output from an NN by propagating an input signal through each layer until the output layer'\n",
    "\n",
    "def forward_pass(network,inputs):\n",
    "    \n",
    "    for layer in network:\n",
    "        \n",
    "        input_output = []\n",
    "        \n",
    "        for neuron in layer:\n",
    "            \n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            \n",
    "            neuron['output'] = ReLu(activation)\n",
    "            \n",
    "            input_output.append(neuron['output'])\n",
    "            \n",
    "        inputs =  input_output\n",
    "        \n",
    "    return inputs \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [3,0,None]\n",
    "output = forward_pass(network,inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in network:\n",
    "    \n",
    "    for neuron in layer:\n",
    "        \n",
    "        print(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BackProp\n",
    "'Our goal is to backpropagate error (to update weights) between the expected and real(the one that is obtained during the forward pass) outputs.We start with computing derivative of the activation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLuDerivative(x):\n",
    "    \n",
    "    return np.greater(x, 0).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error backprop\n",
    "'Since we going back, we start error calculation from the last layer'\n",
    "\n",
    "def back_prop_error(network,expected):\n",
    "    \n",
    "    for i in reversed(range(len(network))):\n",
    "        \n",
    "        layer = network[i]\n",
    "        \n",
    "        errors = list()\n",
    "        \n",
    "        # Go here in the the second iteration\n",
    "        \n",
    "        if i !=len(network)-1:\n",
    "            \n",
    "            for j in range(len(layer)):\n",
    "                \n",
    "                error = 0.0\n",
    "                \n",
    "                for neuron in network[i+1]:\n",
    "                    \n",
    "                    error += (neuron['weights'][j]*neuron['delta'])\n",
    "                    \n",
    "                errors.append(error)\n",
    "          \n",
    "        # During the first iteration we compute error on the last (output) layer of ANN\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for j in range(len(layer)):\n",
    "                \n",
    "                neuron = layer[j]\n",
    "                \n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        \n",
    "        # Move an error 'back' further to the left (closer to the input)        \n",
    "        \n",
    "        for j in range(len(layer)):\n",
    "            \n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j]*ReLuDerivative(neuron['output'])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Now the next step is to train ANN (we need to update weights by applying gradient descent algorithm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with updating weights acc to GD: \n",
    "# weight = weight + learning_rate * error * input\n",
    "\n",
    "def weights_update(network, inputs , learning_rate):\n",
    "    \n",
    "    for i in range(len(network)):\n",
    "        \n",
    "        inputs = inputs[:-1] # Return all exept last one (bias)\n",
    "        \n",
    "        if i!=0:\n",
    "            \n",
    "            inputs = [neuron['output'] for neuron in network[i-1]]\n",
    "            \n",
    "        for neuron in network[i]:\n",
    "            \n",
    "            for j in range(len(inputs)):\n",
    "                \n",
    "                neuron['weights'][j] += learning_rate * neuron['delta'] * inputs[j]\n",
    "                \n",
    "            neuron['weights'][-1] += learning_rate * neuron['delta']\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train Network\n",
    "\n",
    "'We need to make one forward pass, then to compute our error between the real value and what we actually expected (supervise learning approach).  This difference is reflected in a cost function with we are going to optimize by using Gradient Descent.'\n",
    "\n",
    "def train_network(network,dataset,learning_rate, epochs, n_outputs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        cost = 0\n",
    "        \n",
    "        for row in dataset:\n",
    "            \n",
    "            outputs = forward_pass(network,row)\n",
    "            expected = [0 for i in range(n_outputs)] # Define expected class labels\n",
    "            expected[row[-1]] = 1\n",
    "            \n",
    "            cost += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))]) #MSE\n",
    "            \n",
    "            back_prop_error(network,expected)\n",
    "            \n",
    "            weights_update(network,row,learning_rate)\n",
    "            \n",
    "        print('>epoch=%d, lrate=%.3f, cost=%.3f' % (epoch, learning_rate, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset])) # Binary problem, therefore only two outputs\n",
    "learning_rate = 1e-4\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ANN(inputs,2,n_outputs)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_network(network,dataset,learning_rate,epochs,n_outputs)\n",
    "for layer in network:\n",
    "\tprint(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
